<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Jean Paul Barddal</title> <meta name="author" content="Jean Paul Barddal"/> <meta name="description" content="My personal webpage. "/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://jpbarddal.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Jean</span> Paul Barddal </h1> <p class="desc">Graduate Program in Informatics (PPGIa), Pontifícia Universidade Católica do Paraná (PUCPR).</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source> <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> <p>Office 8, Building 8</p> <p>R. Imaculada Conceição, 1155</p> <p>Curitiba, Paraná, Brazil</p> <p>+55 41 3271-1351</p> </div> </div> <div class="clearfix"> <p>I’m an Associate Professor with the Knowledge Discovery and Machine Learning (DCAM) of Graduate Program in Informatics (PPGIa) at the Pontifical Catholic University of Paraná (PUCPR). My research focuses on machine learning, more specifically on data stream mining. More specifically, I’m currently working on the following topics:</p> <ul> <li>Supervised learning: data stream classification and regression</li> <li>Unsupervised learning: data stream clustering and outlier detection</li> <li>Data pre-processing: how to handle high-dimensionality, text data, etc, on streaming data</li> <li>Distributed stream processing</li> </ul> <p>If you’re also interested in any of the topics above, either in theoretical or applied ends, and you would like to pursue a degree or collaborate, feel free to drop me a message using the link below. Also, feel free to check my Lattes CV <a href="http://lattes.cnpq.br/5862618116527136" target="_blank" rel="noopener noreferrer">here</a> (in Portuguese).</p> </div> <div class="news"> <h2>news</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Dec 26, 2024</th> <td> <a class="news-title" href="/news/announcement_35/">IEEE Big Data 2024, Washington, D.C.</a> </td> </tr> <tr> <th scope="row">Aug 10, 2024</th> <td> <a class="news-title" href="/news/announcement_34/">Visiting professor at ETS, Montreal, Canada</a> </td> </tr> <tr> <th scope="row">Aug 9, 2024</th> <td> <a class="news-title" href="/news/announcement_32/">Papers accepted at ICPR and ESWA</a> </td> </tr> <tr> <th scope="row">Jun 17, 2024</th> <td> <a class="news-title" href="/news/announcement_31/">Seminar at Université de Rouen, France</a> </td> </tr> <tr> <th scope="row">Jan 15, 2024</th> <td> <a class="news-title" href="/news/announcement_30/">Paper accepted at Applied Soft Computing</a> </td> </tr> </table> </div> </div> <div class="publications"> <h2>selected publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ACM TIST</abbr></div> <div id="TIST_CD_TEXT_STREAMS:2024" class="col-sm-8"> <div class="title">Concept Drift Adaptation in Text Stream Mining Settings: A Systematic Review</div> <div class="author"> Cristiano Mesquita Garcia, Ramon Abilio, Alessandro Lameiras Koerich, Alceu Souza Britto Jr., and <em>Jean Paul Barddal</em> </div> <div class="periodical"> <em>ACM Transactions on Intelligent Systems and Technology</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2024_ACM_TIST_CD_TEXT_STREAMS.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The society produces textual data online in several ways, e.g., via reviews and social media posts. Therefore, numerous researchers have been working on discovering patterns in textual data that can indicate peoples’ opinions, interests, etc. Most tasks regarding natural language processing are addressed using traditional machine learning methods and static datasets. This setting can lead to several problems, e.g., outdated datasets and models, which degrade in performance over time. This is particularly true regarding concept drift, in which the data distribution changes over time. Furthermore, text streaming scenarios also exhibit further challenges, such as the high speed at which data arrives over time. Models for stream scenarios must adhere to the aforementioned constraints while learning from the stream, thus storing texts for limited periods and consuming low memory. This study presents a systematic literature review regarding concept drift adaptation in text stream scenarios. Considering well-defined criteria, we selected 48 papers published between 2018 and August 2024 to unravel aspects such as text drift categories, detection types, model update mechanisms, stream mining tasks addressed, and text representation methods and their update mechanisms. Furthermore, we discussed drift visualization and simulation and listed real-world datasets used in the selected papers. Finally, we brought forward a discussion on existing works in the area, also highlighting open challenges and future research directions for the community.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">TIST_CD_TEXT_STREAMS:2024</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{ACM}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Garcia, Cristiano Mesquita and Abilio, Ramon and Koerich, Alessandro Lameiras and de Souza Britto Jr., Alceu and Barddal, Jean Paul}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Concept Drift Adaptation in Text Stream Mining Settings: A Systematic Review}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ACM Transactions on Intelligent Systems and Technology}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NEUCOM &amp; APPS</abbr></div> <div id="NEUCOM_AND_APPLICATIONS:2024" class="col-sm-8"> <div class="title">Representation ensemble learning applied to facial expression recognition</div> <div class="author"> Bruna Rossetto Delazeri, André Gustavo Hochuli, <em>Jean Paul Barddal</em>, Alessandro Lameiras Koerich, and Alceu Souza Britto Jr.</div> <div class="periodical"> <em>Neural Computing and Applications</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2024_NEUCOMANDAPPS_REP_ENSEMBLE.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This work introduces the representation ensemble learning algorithm, a novel approach for generating diverse unsupervised representations rooted in the principles of self-taught learning. The ensemble comprises convolutional autoencoders (CAEs) learned in an unsupervised manner, fostering diversity via a loss function designed to penalize similar CAEs’ latent representations. We employ support vector machines, bagging, and random forest as primary classification methods for the final classification step. Additionally, we incorporate KnoraU, a well-established technique used to dynamically select competent classifiers based on a test sample. We evaluate various fusion strategies, including sum, product, and stacking, to comprehensively assess the ensemble’s performance. A robust experimental protocol considering the facial expression recognition problem shows that the proposed approach based on self-taught learning surpasses the accuracy of fine-tuned convolutional neural network (CNN) models. In terms of accuracy, the proposed method is up to 9.9 and 6.3 percentage points better than the CNN-based models fine-tuned for JAFFE and CK+ datasets, respectively.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">NEUCOM_AND_APPLICATIONS:2024</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Delazeri, Bruna Rossetto and Hochuli, Andr\'{e} Gustavo and Barddal, Jean Paul and Koerich, Alessandro Lameiras and de Souza Britto Jr., Alceu}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Representation ensemble learning applied to facial expression recognition}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Neural Computing and Applications}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IEEE BIG DATA</abbr></div> <div id="IEEE_BIG_DATA_IS_IT_FINE:2024" class="col-sm-8"> <div class="title">Is it Fine to Tune? Evaluating SentenceBERT Fine-tuning for Brazilian Portuguese Text Stream Classification</div> <div class="author"> Bruno Yuiti Leão Imai, Cristiano Mesquita Garcia, Marcio Vinicius Rocha, Alessandro Lameiras Koerich, Alceu Souza Britto Jr., and <em>Jean Paul Barddal</em> </div> <div class="periodical"> <em>In IEEE International Conference on Big Data (IEEE Big Data)</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2024_IEEE_BIG_DATA_IS_IT_FINE.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Pre-trained language models (LMs) have been used in several scenarios and data mining tasks due to their good-quality representations and their use readiness. Although LMs constitute a significant gain in usability, they are frequently utilized statically over time, meaning that these models can suffer from concept drift and semantic shift, which correspond to changes in data distribution and word meanings. These phenomena are more noticeable when new texts become gradually available. This paper evaluates the impact of updating pre-trained SentenceBERT models overtime on a Brazilian news post classification task in text streaming fashion, a paradigm suitable for learning from data streams. While we update the SBERT model yearly with a reduced number of recent posts, we compare it with scenarios using static LMs. We used the adaptive random forest for classification and evaluated it regarding macro F1-score and elapsed time. The experimental results show that regularly leveraging sampled texts from the recent past for fine-tuning LMs can improve performance metrics over time, reaching better results than using static LMs in most years analyzed. We also evaluated the run times, which suggests that fine-tuning LMs over time provides a good trade-off between performance and run time.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">IEEE_BIG_DATA_IS_IT_FINE:2024</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Imai, Bruno Yuiti Le\~{a}o and Garcia, Cristiano Mesquita and Rocha, Marcio Vinicius and Koerich, Alessandro Lameiras and de Souza Britto Jr., Alceu and Barddal, Jean Paul}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Is it Fine to Tune? Evaluating SentenceBERT Fine-tuning for Brazilian Portuguese Text Stream Classification}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Big Data (IEEE Big Data)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IEEE BIG DATA</abbr></div> <div id="IEEE_BIG_DATA_LONGKEY:2024" class="col-sm-8"> <div class="title">LongKey: Keyphrase Extraction for Long Documents</div> <div class="author"> Jeovane Honorio Alves, Radu State, Cinthia Obladen Almendra Freitas, and <em>Jean Paul Barddal</em> </div> <div class="periodical"> <em>In IEEE International Conference on Big Data (IEEE Big Data)</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2024_IEEE_BIG_DATA_LONGKEY.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In an era of information overload, manually annotating the vast and growing corpus of documents and scholarly papers is increasingly impractical. Automated keyphrase extraction addresses this challenge by identifying representative terms within texts. However, most existing methods focus on short documents (up to 512 tokens), leaving a gap in processing long-context documents. In this paper, we introduce LongKey, a novel framework for extracting keyphrases from lengthy documents, which uses an encoder-based language model to capture extended text intricacies. LongKey uses a max-pooling embedder to enhance keyphrase candidate representation. Validated on the comprehensive LDKP datasets and six diverse, unseen datasets, LongKey consistently outperforms existing unsupervised and language model-based keyphrase extraction methods. Our findings demonstrate LongKey’s versatility and superior performance, marking an advancement in keyphrase extraction for varied text lengths and domains.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">IEEE_BIG_DATA_LONGKEY:2024</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Alves, Jeovane Honorio and State, Radu and de Almendra Freitas, Cinthia Obladen and Barddal, Jean Paul}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LongKey: Keyphrase Extraction for Long Documents}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Big Data (IEEE Big Data)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICMLA</abbr></div> <div id="ICMLA_FUEL_PRED:2024" class="col-sm-8"> <div class="title">Fuels Demand Forecasting: Identifying Leading Feature Sets, Prediction Strategy, and Regressors</div> <div class="author"> Jonas Krause, Alexandre C. A. Beiruth, <em>Jean Paul Barddal</em>, Alceu Souza Britto Jr, and Vinicius Mourão Alves Souza</div> <div class="periodical"> <em>In International Conference on Machine Learning and Applications (ICMLA)</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2024_ICMLA_FUEL_PREDICTION.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Fuels are crucial for any country’s development and economy, impacting various sectors such as transportation, industry, and electricity generation. Accurate prediction of monthly fuel demand can improve supply chain management, strategic decision-making, and financial planning for businesses while helping governments develop decarbonization policies and estimate pollutant emissions. This paper explores machine learning models to forecast fossil fuels and biofuel demand 12 months ahead, using univariate time series data representing the historical sales of 27 Brazilian states, one of the world’s leading producers and consumers of fuels. We evaluate different time series feature sets, machine learning regression models, and prediction strategies to address the complexity of fuel sales influenced by factors such as economic conditions and geopolitical events. Our comprehensive evaluation aims to determine an effective setting for predictive models in the fuel domain. Our results show that popular feature extractors for time series, such as Catch22 and TsFresh, cannot improve the original data representation for most forecasting models. Although focused on Brazil, our findings apply to other countries, since the trained models do not rely on external variables, such as micro and macroeconomic indicators.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ICMLA_FUEL_PRED:2024</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Krause, Jonas and Beiruth, Alexandre C. A. and Barddal, Jean Paul and de Souza Britto Jr, Alceu and Souza, Vinicius Mourão Alves}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fuels Demand Forecasting: Identifying Leading Feature Sets, Prediction Strategy, and Regressors}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning and Applications (ICMLA)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICPR</abbr></div> <div id="ICPR_CATASTROPHIC:2024" class="col-sm-8"> <div class="title">Alleviating Catastrophic Forgetting in Facial Expression Recognition with Emotion-Centered Models</div> <div class="author"> Israel A. Laurensi, Alceu Souza Britto Jr., <em>Jean Paul Barddal</em>, and Alessandro Lameiras Koerich</div> <div class="periodical"> <em>In International Conference on Pattern Recognition (ICPR)</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2024_ICPR_ALLEVIATING_CATASTROPHIC.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Facial expression recognition is pivotal in machine learning, facilitating various applications. However, convolutional neural networks (CNNs) are often plagued by catastrophic forgetting, impeding their adaptability. The proposed method, emotion-centered generative replay (ECgr), tackles this challenge by integrating synthetic images from generative adversarial networks. Moreover, ECgr incorporates a quality assurance algorithm to ensure the fidelity of generated images. This dual approach enables CNNs to retain past knowledge while learning new tasks, enhancing their performance in emotion recognition. The experimental results on four diverse facial expression datasets demonstrate that incorporating images generated by our pseudo-rehearsal method enhances training on the targeted dataset and the source dataset while making the CNN retain previously learned knowledge.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ICPR_CATASTROPHIC:2024</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Laurensi, Israel A. and de Souza Britto Jr., Alceu and Barddal, Jean Paul and Koerich, Alessandro Lameiras}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Alleviating Catastrophic Forgetting in {Facial} Expression Recognition with Emotion-Centered Models}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Pattern Recognition (ICPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICPR</abbr></div> <div id="ICPR_SAMPLING:2024" class="col-sm-8"> <div class="title">Improving Sampling Methods for Fine-tuning SentenceBERT in Text Streams</div> <div class="author"> Cristiano Mesquita Garcia, Alessandro Lameiras Koerich, Alceu Souza Britto Jr., and <em>Jean Paul Barddal</em> </div> <div class="periodical"> <em>In International Conference on Pattern Recognition (ICPR)</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2024_ICPR_SAMPLING_TEXT.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The proliferation of textual data on the Internet presents a unique opportunity for institutions and companies to monitor public opinion about their services and products. Given the rapid generation of such data, the text stream mining setting, which handles sequentially arriving, potentially infinite text streams, is often more suitable than traditional batch learning. While pre-trained language models are commonly employed for their high-quality text vectorization capabilities in streaming contexts, they face challenges adapting to concept drift—the phenomenon where the data distribution changes over time, adversely affecting model performance. Addressing the issue of concept drift, this study explores the efficacy of seven text sampling methods designed to fine-tune language models, thereby mitigating performance degradation selectively. We precisely assess the impact of these methods on fine-tuning the SBERT model using four different loss functions. Our evaluation, focused on Macro F1-score and elapsed time, employs two text stream datasets and an incremental SVM classifier to benchmark performance. Our findings indicate that Softmax loss and Batch All Triplets loss are particularly effective for text stream classification, demonstrating that larger sample sizes correlate with improved macro F1 scores. Notably, our proposed WordPieceToken ratio sampling method significantly enhances performance with the identified loss functions, surpassing baseline results.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ICPR_SAMPLING:2024</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Garcia, Cristiano Mesquita and Koerich, Alessandro Lameiras and de Souza Britto Jr., Alceu and Barddal, Jean Paul}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Improving Sampling Methods for Fine-tuning SentenceBERT in Text Streams}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Pattern Recognition (ICPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ESWA</abbr></div> <div id="ESWA_GARCIA:2024" class="col-sm-8"> <div class="title">Temporal analysis of drifting hashtags in textual data streams: A graph-based application</div> <div class="author"> Cristiano Mesquita Garcia, Alceu Souza Britto Jr., and <em>Jean Paul Barddal</em> </div> <div class="periodical"> <em>Expert Systems with Applications</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2024_TEMPORAL_GRAPH.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Initially supported by Twitter, hashtags are now used on several social media platforms. Hashtags are helpful for tagging, tracking, and grouping posts on similar topics. In this paper, based on a hashtag stream regarding the hashtag #mybodymychoice, we analyze hashtag drifts over time using concepts from graph analysis and textual data streams using the Girvan–Newman method to uncover hashtag communities in annual snapshots between 2018 and 2022. In addition, we offer insights about some correlated hashtags found in the study. Our approach can be useful for monitoring changes over time in opinions and sentiment patterns about an entity on social media. Even though the hashtag #mybodymychoice was initially coupled with women’s rights, abortion, and bodily autonomy, we observe that it suffered drifts during the studied period across topics such as drug legalization, vaccination, political protests, war, and civil rights. The year 2021 was the most significant drifting year, in which the communities detected and their respective sizes suggest that #mybodymychoice had a significant drift to vaccination and Covid-19-related topics.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ESWA_GARCIA:2024</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Garcia, Cristiano Mesquita and de Souza Britto Jr., Alceu and Barddal, Jean Paul}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Temporal analysis of drifting hashtags in textual data streams: A graph-based application}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Expert Systems with Applications}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">APL. SOFT. COMP.</abbr></div> <div id="ASOC_TIEPPO:2023" class="col-sm-8"> <div class="title">Adaptive Learning on Hierarchical Data Streams using Window-weighted Gaussian Probabilities</div> <div class="author"> Eduardo Tieppo, Julio Cesar Nievola, and <em>Jean Paul Barddal</em> </div> <div class="periodical"> <em>Applied Soft Computing</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/2024_TIEPPO_ASOC_GNB.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The hierarchical data stream classification task addresses challenges in both hierarchical and data stream classification primary areas. In these scenarios, machine learning models must simultaneously deal with class hierarchies and adapt to respond to nonstationary data. Given such a challenging set of traits, existing techniques are deficient, as they perform incremental learning and are slow to adapt to newer data, thus not capturing their dynamics in a timely fashion. In this study, we propose two novel adaptive Gaussian Naive Bayes classifiers tailored to classify hierarchical data streams. The models use window-weighted Gaussian probabilities to consider current and historical data and improve the adaptability of the classifiers, especially for nonstationary data streams. As a result of our research, we introduce a unified protocol for evaluating and comparing hierarchical data stream classifiers and establish a benchmark for the hierarchical data stream classification task encompassing the proposed methods and state-of-the-art classifiers. The results demonstrate that our proposed algorithms achieve better prediction correctness than their state-of-the-art counterparts while responding more swiftly to changes in data distribution.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ASOC_TIEPPO:2023</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tieppo, Eduardo and Nievola, Julio Cesar and Barddal, Jean Paul}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Adaptive Learning on Hierarchical Data Streams using Window-weighted Gaussian Probabilities}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Applied Soft Computing}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6A%70%62%61%72%64%64%61%6C@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://orcid.org/0000-0001-9928-854X" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=ir58bWkAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://www.researchgate.net/profile/Jean-Paul-Barddal/" title="ResearchGate" target="_blank" rel="noopener noreferrer"><i class="ai ai-researchgate"></i></a> <a href="https://github.com/jpbarddal" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/jean-paul-b-0a076895" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://dblp.org/pid/148/4305.html" title="DBLP" target="_blank" rel="noopener noreferrer"><i class="ai ai-dblp"></i></a> </div> <div class="contact-note"> </div> </div> </article> <center> <a href="https://info.flagcounter.com/hFFA" target="_blank" rel="noopener noreferrer"><img src="https://s01.flagcounter.com/countxl/hFFA/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_15/viewers_0/labels_1/pageviews_0/flags_0/percent_1/" alt="Flag Counter" border="0"></a> </center> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Jean Paul Barddal. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-L588BBLF3F"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-L588BBLF3F");</script> </body> </html>